# W&B Weave for Agent Self-Improvement

This doc describes how to use W&B Weave so your research and map agents can **self-improve** via tracing, feedback, evaluation, and prompt/memory updates.

**Two improvement cycles:** There is also a **separate** improvement cycle that does **not** use Weave (backlog + user submissions â†’ code PRs and/or persona updates). See [IMPROVEMENT_CYCLE.md](./IMPROVEMENT_CYCLE.md).

## What You Have Today

- **Tracing**: server-nestâ€™s `TracingService` and workflow/agent tracing wrap key operations in Weave ops when `WANDB_API_KEY` is set. Each Letta API call is one Weave trace.
- **Agents**: Research agent (web search, save research) and map agent (layers, view) share a block; personas live in Letta memory blocks.

Self-improvement = **observe** (traces) â†’ **judge** (feedback + evals) â†’ **update** (personas, memory, few-shot examples).

---

## 1. Trace One User Task as One Run (Recommended)

Right now each `sendMessage` and `updateBlock` is a separate trace. For improvement you want **one trace per user task** (e.g. one â€œfind museums in Portlandâ€ â†’ research â†’ block update â†’ map).

**Option A â€“ Workflow endpoint (recommended)**  
Add a single endpoint that runs the full workflow (research agent â†’ update block â†’ map agent) and wrap it in one Weave op (e.g. using server-nestâ€™s `TracingService.trace()`). Then:

- One Weave trace = one user query and its full outcome.
- Feedback and evals attach to that trace (e.g. â€œdid the map show the right area?â€).

**Option B â€“ Keep current API**  
Keep separate `POST /api/agents/:id/messages` and `POST /api/workflow/update-block`. Each trace is one Letta call. You can still add feedback and evals per message; improvement is then per â€œresearch replyâ€ or â€œmap replyâ€ rather than per full task.

---

## 2. Collect Feedback (Signals for Improvement)

Weave supports:

- **Human feedback in the UI**: On each trace, reviewers add ğŸ‘/ğŸ‘, notes, or **human-annotation scorers** (e.g. â€œDid the research include coordinates?â€). Use this to curate good/bad examples.
- **Programmatic feedback**: When your app has a signal (e.g. â€œuser accepted mapâ€, â€œuser re-askedâ€), add feedback to the trace. The Weave **Python** SDK supports `call.feedback.add_reaction()`, `add_note()`, `add("correctness", {value: 5})`. The **Node/JS** SDKâ€™s feedback APIs are not yet as complete; use the **Service API** or a small Python helper to attach feedback by call ID if needed.
- **Scorers (evals)**: Run automated scorers on traces (see below); scores are stored as feedback on the call.

Use feedback to:
- Decide which traces are â€œgoodâ€ (e.g. thumbs up, score &gt; threshold) vs â€œbadâ€ (thumbs down, low score).
- Build datasets of good/bad examples for prompt updates or few-shot examples.

---

## 3. Evaluate with Scorers (Automated Quality)

Weave **scorers** evaluate model/agent output and attach scores to traces. They are the main lever for **automated** self-improvement.

- **Where**: Scorers are defined and run in **Python** (Weaveâ€™s `weave.Evaluation`, `@weave.op` scorers, or `call.apply_scorer()`).
- **What to score** (examples):
  - **Research agent**: â€œDoes the research summary include place names and [lng, lat] or addresses?â€ (e.g. regex + simple LLM judge); â€œDid it call `save_research`?â€
  - **Map agent**: â€œDid it call `set_map_view`?â€; â€œDid it add at least one layer?â€; â€œDo tool calls contain valid GeoJSON?â€
- **How**:
  1. **Eval pipeline**: A Python job (script or cron) that:
     - Fetches recent traces from Weave (e.g. `client.get_calls()` with filters), or reads from a queue fed by your server.
     - For each trace, extracts the relevant output (e.g. last message, tool calls).
     - Runs your scorers (function or class-based `weave.Scorer`).
     - Writes results back (e.g. `call.apply_scorer(scorer)` or log to Weave as feedback).
  2. **Evaluation + dataset**: Build a Weave **dataset** of example inputs (e.g. â€œmuseums in Portlandâ€), run your agent (or a stub that returns stored outputs) for each row, run scorers, and use Weaveâ€™s Evaluation UI to compare runs and track metrics over time.

Put the Python eval script in `weave-eval/` (or similar), use `weave.init(project_name)` and the same W&B project as the Node server so traces and scores live together.

---

## 4. Use Feedback and Scores to Improve Agents

â€œSelf-improveâ€ here means **updating agent behavior** using Weave data, not necessarily fully automated RL.

- **Prompt / persona updates**  
  - Periodically (e.g. weekly) open Weave, filter traces by low scores or ğŸ‘.  
  - Inspect failures (e.g. â€œresearch never included coordinatesâ€, â€œmap agent didnâ€™t set viewâ€).  
  - Edit `RESEARCH_PERSONA` / `MAP_PERSONA` in `server-nest/src/workflow/workflow.config.ts` (or in Letta UI) to add instructions or constraints.  
  - Deploy and let new traces show up in Weave; compare old vs new runs.

- **Few-shot / memory**  
  - Export high-scoring or ğŸ‘ traces from Weave (e.g. â€œuser queryâ€ + â€œgood research summaryâ€ or â€œgood tool sequenceâ€).  
  - Add them as few-shot examples in the agentâ€™s system prompt or as Letta memory blocks so the agent sees good patterns.

- **Guardrails / monitors (optional)**  
  - Weave supports **guardrails** (modify input/output in real time) and **monitors** (score production traces). You can add a scorer that flags e.g. â€œno coordinates in researchâ€ and then either block the reply or send it to a human; over time, use the same scorerâ€™s trends to decide prompt changes.

---

## 5. Minimal Implementation Checklist

1. **Keep/enable tracing**  
   Set `WANDB_API_KEY` so every `sendMessage` and `updateBlock` is traced.

2. **Optional: one trace per workflow**  
   Add a `POST /api/workflow/run` (or similar) that runs research â†’ update block â†’ map in one go, and wrap that in a single Weave op so one trace = one user task.

3. **Human feedback**  
   Use the Weave UI to add ğŸ‘/ğŸ‘ and notes (and human-annotation scorers) on traces. Use these to pick examples for prompt edits.

4. **Automated evals**  
   Add a Python script (e.g. `weave-eval/run_scorers.py`) that:
   - Uses `weave.init("weave-hacks")` (same project as server).
   - Fetches recent calls (or reads from a queue).
   - Runs 1â€“2 simple scorers (e.g. â€œresearch_has_coordinatesâ€, â€œmap_has_set_viewâ€).
   - Writes scores back via `call.apply_scorer(...)` or equivalent.
   Run it on a schedule or after each workflow run.

5. **Improvement loop**  
   Regularly review low-scoring and ğŸ‘ traces in Weave, update `RESEARCH_PERSONA` / `MAP_PERSONA` (or memory blocks) in `agents-store.js`, and redeploy. Optionally add good examples as few-shot or memory.

---

## 6. References

- [Weave: Collect feedback and use annotations](https://weave-docs.wandb.ai/guides/tracking/feedback)
- [Weave: Scoring overview](https://docs.wandb.ai/weave/guides/evaluation/scorers)
- [Weave: Evaluations and datasets](https://docs.wandb.ai/weave/guides/core-types/evaluations)
- [Weave: Guardrails and monitors](https://docs.wandb.ai/weave/guides/evaluation/guardrails_and_monitors)
